{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQyICiCdZG3R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/Bengaluru_house_price_cleaned.csv')\n",
        "\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\nBasic Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# ==================== DATA PREPROCESSING ====================\n",
        "\n",
        "# Handle missing values\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DATA PREPROCESSING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Fill missing values in 'society' with 'Unknown'\n",
        "df['society'] = df['society'].fillna('Unknown')\n",
        "\n",
        "# Fill missing bath and balcony with median\n",
        "df['bath'] = df['bath'].fillna(df['bath'].median())\n",
        "df['balcony'] = df['balcony'].fillna(df['balcony'].median())\n",
        "\n",
        "# Handle total_sqft - convert ranges to average\n",
        "def convert_sqft(x):\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    x = str(x).strip()\n",
        "    if '-' in x:\n",
        "        parts = x.split('-')\n",
        "        return (float(parts[0]) + float(parts[1])) / 2\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "df['total_sqft'] = df['total_sqft'].apply(convert_sqft)\n",
        "df['total_sqft'] = df['total_sqft'].fillna(df['total_sqft'].median())\n",
        "\n",
        "# Extract number of bedrooms from 'size' column\n",
        "def extract_bhk(x):\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    x = str(x).strip()\n",
        "    if 'BHK' in x or 'RK' in x:\n",
        "        return int(x.split()[0])\n",
        "    elif 'Bedroom' in x:\n",
        "        return int(x.split()[0])\n",
        "    else:\n",
        "        return np.nan\n",
        "\n",
        "df['bhk'] = df['size'].apply(extract_bhk)\n",
        "df['bhk'] = df['bhk'].fillna(df['bhk'].median())\n",
        "\n",
        "# Create price per sqft feature\n",
        "df['price_per_sqft'] = df['price'] * 100000 / df['total_sqft']  # Price in lakhs\n",
        "\n",
        "# Remove outliers based on price per sqft\n",
        "df = df[(df['price_per_sqft'] >= df['price_per_sqft'].quantile(0.01)) &\n",
        "        (df['price_per_sqft'] <= df['price_per_sqft'].quantile(0.99))]\n",
        "\n",
        "# Remove outliers based on bhk and total_sqft relationship\n",
        "df = df[df['total_sqft']/df['bhk'] >= 300]  # At least 300 sqft per bedroom\n",
        "\n",
        "print(f\"\\nDataset shape after cleaning: {df.shape}\")\n",
        "\n",
        "# ==================== FEATURE ENGINEERING ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Select important categorical features\n",
        "categorical_features = ['location', 'area_type', 'availability', 'zone_name']\n",
        "numerical_features = ['total_sqft', 'bath', 'balcony', 'bhk', 'price_per_sqft']\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for col in categorical_features:\n",
        "    le = LabelEncoder()\n",
        "    df[col + '_encoded'] = le.fit_transform(df[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Prepare features and target\n",
        "feature_cols = [col + '_encoded' for col in categorical_features] + numerical_features\n",
        "X = df[feature_cols]\n",
        "y = df['price']\n",
        "\n",
        "print(f\"Features: {X.columns.tolist()}\")\n",
        "print(f\"Target: price (in lakhs)\")\n",
        "print(f\"Number of samples: {len(X)}\")\n",
        "\n",
        "# ==================== TRAIN-TEST SPLIT ====================\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ==================== MODEL TRAINING ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL TRAINING & EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge Regression': Ridge(alpha=10),\n",
        "    'Lasso Regression': Lasso(alpha=1),\n",
        "    'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=42),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Train model\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_train = model.predict(X_train_scaled)\n",
        "    y_pred_test = model.predict(X_test_scaled)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    train_r2 = r2_score(y_train, y_pred_train)\n",
        "    test_r2 = r2_score(y_test, y_pred_test)\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
        "\n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train_scaled, y_train,\n",
        "                                cv=5, scoring='r2')\n",
        "\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Train R¬≤': train_r2,\n",
        "        'Test R¬≤': test_r2,\n",
        "        'RMSE': test_rmse,\n",
        "        'MAE': test_mae,\n",
        "        'CV R¬≤ (mean)': cv_scores.mean(),\n",
        "        'CV R¬≤ (std)': cv_scores.std()\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Train R¬≤: {train_r2:.4f}\")\n",
        "    print(f\"  Test R¬≤: {test_r2:.4f}\")\n",
        "    print(f\"  RMSE: {test_rmse:.2f} lakhs\")\n",
        "    print(f\"  MAE: {test_mae:.2f} lakhs\")\n",
        "    print(f\"  CV R¬≤ Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "\n",
        "# Results summary\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Select best model (highest test R¬≤)\n",
        "best_model_name = results_df.loc[results_df['Test R¬≤'].idxmax(), 'Model']\n",
        "best_model = models[best_model_name]\n",
        "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
        "\n",
        "# ==================== FEATURE IMPORTANCE ====================\n",
        "\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FEATURE IMPORTANCE (Top 10)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': feature_cols,\n",
        "        'Importance': best_model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    print(feature_importance.head(10).to_string(index=False))\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    top_features = feature_importance.head(10)\n",
        "    plt.barh(top_features['Feature'], top_features['Importance'])\n",
        "    plt.xlabel('Importance')\n",
        "    plt.title(f'Top 10 Feature Importances - {best_model_name}')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\nFeature importance plot saved as 'feature_importance.png'\")\n",
        "\n",
        "# ==================== PREDICTIONS VISUALIZATION ====================\n",
        "\n",
        "y_pred_final = best_model.predict(X_test_scaled)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Actual vs Predicted\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_test, y_pred_final, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
        "         'r--', lw=2, label='Perfect Prediction')\n",
        "plt.xlabel('Actual Price (lakhs)')\n",
        "plt.ylabel('Predicted Price (lakhs)')\n",
        "plt.title('Actual vs Predicted Prices')\n",
        "plt.legend()\n",
        "\n",
        "# Residuals\n",
        "plt.subplot(1, 2, 2)\n",
        "residuals = y_test - y_pred_final\n",
        "plt.scatter(y_pred_final, residuals, alpha=0.5)\n",
        "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "plt.xlabel('Predicted Price (lakhs)')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('predictions.png', dpi=300, bbox_inches='tight')\n",
        "print(\"\\nPredictions plot saved as 'predictions.png'\")\n",
        "\n",
        "# ==================== SAMPLE PREDICTIONS ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SAMPLE PREDICTIONS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "sample_indices = np.random.choice(len(X_test), 5, replace=False)\n",
        "for idx in sample_indices:\n",
        "    actual = y_test.iloc[idx]\n",
        "    predicted = y_pred_final[idx]\n",
        "    error = abs(actual - predicted)\n",
        "    print(f\"Actual: ‚Çπ{actual:.2f} lakhs | Predicted: ‚Çπ{predicted:.2f} lakhs | Error: ‚Çπ{error:.2f} lakhs\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL TRAINING COMPLETE!\")\n",
        "print(\"=\"*50)"
      ]
    }
  ]
}